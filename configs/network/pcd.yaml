# @package _global_
network:
  # trunk transformer config
  _target_: ppt_learning.models.policy.Policy
  embed_dim: 128
  num_blocks: 4 # num of blocks in the trunk transformer 
  num_heads: 8 # num of heads in the trunk transformer
  drop_path: 0.0 # drop path in the trunk transformer
  use_modality_embedding: True
  token_postprocessing: ${head.token_postprocessing}
  cross_stem_attention: False # use cross attention to combine state and action, not used
  weight_init_style: 'pytorch' # weight init
  observation_horizon: ${dataset.observation_horizon}
  action_horizon: ${dataset.action_horizon}
  openloop_steps: 4
  action_dim : -1 # overwrite based on dataset
  temporal_agg: False # ACT-style temporal aggregation, not always work

# head network (MLP, Diffusion, TransformerDecoder)
# head:
#     _target_: ppt_learning.models.policy_head.MLP
#     input_dim: ${network.embed_dim}
#     tanh_end: True # normalized action output
#     output_dim: -1 # overwrite based on dataset
#     widths: [512, 256]
#     normalize_action: ${head.tanh_end}
#     token_postprocessing: "mean"

head:
  _target_: ppt_learning.models.policy_head.Diffusion
  input_dim: ${network.embed_dim} # concat = 11264
  output_dim: -1 # overwrite based on dataset
  horizon: ${dataset.action_horizon}
  normalize_action: True
  down_dims: [128, 256, 512]
  noise_scheduler_type: "DDIM"
  num_inference_steps: 10
  token_postprocessing: "mean" # "concat" # maxpool or meanpool the tokens
  hist_horizon: 0 # whether predicting the hist action
  
# head:
#     _target_: ppt_learning.models.policy_head.TransformerDecoder
#     token_dim: ${network.embed_dim}
#     output_dim: -1 # overwrite based on dataset
#     horizon: ${dataset.action_horizon}
#     tanh_end: True
#     normalize_action: ${head.tanh_end}
#     token_postprocessing: ""

stem:
  modalities: ['state', 'pointcloud'] # no 'language'
  modality_embed_dim: ${network.embed_dim}
  normalize_state: True # normalize state vectors 
  num_heads: 8 # num of heads in the perceiver transformer
  dim_head: 32 # dim of head in the perceiver transformer
  # state_embedding_dim: 32 # dimension of positional encoding for state
  cross_attention: True # whether to use perceiver cross attention or not

  # used as perceiver io to unify token sizes for each modality
  crossattn_latent:
    pointcloud: 1
    state: 1

  pointcloud:
    _target_: ppt_learning.models.policy_stem.PointNet
    pcd_domain: 'dense' # 'scanobjectnn'
    finetune: True
    cfg_name: 'pointnext-s' # We need output_dim to be equal to `modality_embed_dim`, it happens to be 512 in this case so we do not need anything else
    pretrained_path: "/mnt/bn/robot-minghuan-debug/ppt_learning/pretrained_weights/scanobjectnn-pointnext-s_best.pth" # path/to/pretrained/pointnet.pth
    output_dim: ${network.embed_dim}
    # cfg_name: 'pointvector-s' # We need output_dim to be equal to `modality_embed_dim`, it happens to be 512 in this case so we do not need anything else
    # pretrained_path: "pretrained_weights/scanobjectnn-train-pointvector-s-ngpus1-seed4677-20230327-151936-XVPsr4boZEJ2dG2zfrH94T_ckpt_best.pth"

  # pointcloud:
  #   _target_: ppt_learning.models.policy_stem.PointNet
  #   pcd_domain: 'shapenetpart'
  #   cfg_name: 'pointnext-s_c64' # We need output_dim to be equal to `modality_embed_dim`, it happens to be 512 in this case so we do not need anything else
  #   finetune: True
  #   pretrained_path: "shapenetpart-train-pointnext-s_c64-ngpus4-seed7798-20220822-024210-ZcJ8JwCgc7yysEBWzkyAaE_ckpt_best.pth" # path/to/pretrained/pointnet.pth

  # pointcloud:
  #   _target_: ppt_learning.models.policy_stem.DP3PointNetEncoderXYZ # All other paras remains default
  #   pcd_domain: 'scanobjectnn' # just work for dataset preprocessing style
  #   in_channels: ${dataset.pcd_channels}
  #   out_channels: ${stem.modality_embed_dim}

  # implement tokenization for state
  state:
    _target_: ppt_learning.models.policy_stem.MLP
    input_dim: 0 # ovewrite based on the dataset
    output_dim: ${network.embed_dim}
    widths: [128, 256]

  # language:
  #   _target_: ppt_learning.models.policy_stem.TextEncoder
  #   pretrained_model_name_or_path: "all-MiniLM-L6-v2" # https://www.sbert.net/docs/pretrained_models.html
  #   modality_embed_dim: ${stem.modality_embed_dim}

  # image: # camera
  #   _target_: ppt_learning.models.policy_stem.VisionTransformer
