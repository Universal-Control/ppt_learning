seed: 42
output_dir: ${hydra:runtime.output_dir}
wb_tag: ${suffix}
log_interval: 10
script_name: ''
pretrained_dir: ${train.pretrained_dir}
parallel_eval: false
env_names: [
  "Isaac-UR5-PutBowlInMicroWaveAndClose-Joint-Mimic-v0"
]
debug: False # Turn off to allow wandb logging
suffix: "" # suffix for the experiment

defaults:
  - env: isaacsim # rlbench # gensim2
  - network: rgb
  - datasetsize: all
  - _self_

train:
  total_epochs: 250  # maximum training epochs before termination. usually set as maximum
  total_iters: 100000000 # maximum training steps before termination
  epoch_iters: 1000  # training steps in each epoch
  validation_iters: 100 # maximum iterations for validation
  pretrained_dir: <path to your pretrained model folder>
  model_names: 
    - model.pth
  freeze_trunk: False # whether to freeze the trunk during finetuning
prompt: ""
rollout_runner:
  save_video: true
  task_name: "Isaac-UR5-CloseMicroWave-Mimic-Joint-v0"
eval_log_name: "eval_log_l40_8_100_ddp_new"
n_procs: 8
stem:
  pointcloud: 
    pretrained_path: <path to pointnet pretrained model>
