# Unified Evaluation Configuration for PPT Learning
# Supports both sequential and parallel evaluation modes

defaults:
  - base_config
  - _self_

# Evaluation mode: "sequential", "parallel", or "auto"
eval_mode: auto

# Number of parallel processes (only used in parallel mode)
n_procs: 4

# Evaluation settings
eval_log_name: "evaluation_results"
suffix: "unified_eval"

# Training configuration (for compatibility)
train:
  total_epochs: 0  # 0 indicates evaluation mode
  pretrained_dir: "outputs/models"
  model_names:
    - "model_best.pth"
    - "model_latest.pth"

# Domain configuration
domains: "example_domain"
state_dim: 14
action_dim: 7

# Output directory
output_dir: "outputs/eval"

# Network architecture
network:
  _target_: ppt_learning.models.policy.Policy
  embed_dim: 1024
  num_blocks: 24
  num_heads: 16
  use_modality_embedding: true
  temporal_agg: false
  max_timesteps: 1300

# Stem configuration (observation processing)
stem:
  modalities: ["state", "pointcloud"]
  state:
    _target_: ppt_learning.models.policy_stem.StateOnlyModule
    input_dim: 14  # Will be set dynamically
    embed_dim: 1024
  pointcloud:
    _target_: ppt_learning.models.policy_stem.PointcloudModule
    pcd_domain: "scanobjectnn"
    embed_dim: 1024
  cross_attention: false
  modality_embed_dim: 1024

# Head configuration (action prediction)
head:
  _target_: ppt_learning.models.policy_head.DiffusionHead
  output_dim: 7  # Will be set dynamically
  embed_dim: 1024
  normalize_action: true

# Dataset configuration (for validation)
dataset:
  observation_horizon: 4
  action_horizon: 1

# Rollout runner configuration
rollout_runner:
  _target_: ppt_learning.rollout.sim_rollout_runner.SimRolloutRunner
  max_timestep: 1300
  num_episodes: 50
  save_video: false
  hist_action_cond: false
  
  # Point cloud configuration (set dynamically if needed)
  pcdnet_pretrain_domain: "scanobjectnn"
  pcd_channels: 4

# Optimizer configuration (for compatibility)
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4
  weight_decay: 1e-6

# Scheduler configuration (for compatibility) 
lr_scheduler:
  _target_: transformers.get_cosine_schedule_with_warmup
  num_warmup_steps: 500
  num_training_steps: 100000

# Warmup configuration (for compatibility)
warmup_lr:
  step: 500
  lr: 1e-6