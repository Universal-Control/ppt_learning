seed: 42
output_dir: outputs/depth/${now:%Y-%m-%d_%H-%M-%S}
log_interval: 10
script_name: ''
pretrained_dir: ''
parallel_eval: false
env_names: [
    "Isaac-UR5-PutBowlInMicroWaveAndClose-Joint-Mimic-Retry-v0"
  ]
debug: False # Turn off to allow wandb logging
suffix: "" # suffix for the experiment
wb_tag: ${suffix}

batch_size: 512 # batch size

defaults:
  - env: isaacsim # rlbench # gensim2
  - network: depth
  - datasetsize: all
  - _self_

domains: 624_x7_depth_small_change_obs, 624_x7_depth_small_change_obs2
save_interval: 10
dataset_path: /mnt/bn/robot-minghuan-datasets-lq/xiaoshen/datasets/arxx7_picktoothpasteputincup

dataset:
  _target_: ppt_learning.dataset.multi_sim_traj_dataset.MultiTrajDataset
  ignored_keys: ["language", "initial_state", "states", "images", "color", "abs_gripper_pos", "pointcloud", "wbc_target", "wbc_step", "last_action"]
  norm_depth: false
  augment_depth: true
  use_lru_cache: true
  action_horizon: 16 # observation: (observation + action) is action horizon 
  observation_horizon: 1 # before observation horizon is observation
  state_keys: ["base_vel", "left_arm_joint_pos", "right_arm_joint_pos", "lift_pos", "eef_pos_l", "eef_quat_l", "eef_pos_r", "eef_quat_r", "left_normalized_gripper_pos", "right_normalized_gripper_pos"]

dataloader:
  batch_size: ${batch_size}
  num_workers: 15
  pin_memory: true
  persistent_workers: true
  shuffle: true
  drop_last: true
val_dataloader:
  batch_size: ${batch_size}
  num_workers: 2
  shuffle: false
  pin_memory: false
  persistent_workers: false
  drop_last: true
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  eps: 1.0e-08
  weight_decay: 1.0e-06
  betas:
  - 0.95
  - 0.999
warmup_lr:
  lr: 1.0e-03
  step: 500
lr_scheduler: "cosine"
# lr_scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#   T_0: 250
#   T_mult: 1
#   eta_min: 1.0e-08
train:
  total_epochs: 2000 # maximum training epochs before termination. usually set as maximum
  total_iters: 1000000000 # maximum training steps before termination
  epoch_iters: 500  # training steps in each epoch
  validation_iters: 100 # maximum iterations for validation
  pretrained_dir: "/mnt/bn/robot-minghuan-debug/ppt_learning/outputs/depth/2025-06-25_15-33-06/arxx7_picktoothpasteputincup/624_x7_depth_small_change_obs" # pretrained model path
  # pretrained_dir: "" # pretrained model path
  freeze_trunk: False # whether to freeze the trunk during finetuning
  last_k_checkpoints: 5 # number of last checkpoints to keep