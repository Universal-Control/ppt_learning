seed: 42
output_dir: ${hydra:runtime.output_dir}
wb_tag: ${suffix}
log_interval: 10
script_name: ''
pretrained_dir: ${train.pretrained_dir}
parallel_eval: false
env_names: [
    "Isaac-UR5-PutBowlInMicroWaveAndClose-Joint-Mimic-v0"
  ]
debug: False # Turn off to allow wandb logging
suffix: "" # suffix for the experiment

defaults:
  - env: isaacsim_pcd_wbc # rlbench # gensim2
  - network: pcd
  - datasetsize: all
  - _self_

# self-explanatory torch config
network:
  openloop_steps: 8
  temporal_agg: False
dataloader:
  batch_size: ${batch_size}
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  shuffle: true
  drop_last: true
val_dataloader:
  batch_size: ${batch_size}
  num_workers: 8
  shuffle: false
  pin_memory: true
  persistent_workers: true
  drop_last: true
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  eps: 1.0e-08
  weight_decay: 1.0e-06
  betas:
  - 0.95
  - 0.999
warmup_lr:
  lr: 1.0e-06
  step: 500
lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${train.total_iters}
  eta_min: 1.0e-08
rollout_runner:
  save_video: true
  task_name: "Isaac-UR5-PutBowlInMicroWaveAndClose-Joint-Mimic-v0"
  eval_log_name: "eval_log_l40_8_100_wbc"
train:
  total_epochs: 250  # maximum training epochs before termination. usually set as maximum
  total_iters: 100000000 # maximum training steps before termination
  epoch_iters: 1000  # training steps in each epoch
  validation_iters: 100 # maximum iterations for validation
  pretrained_dir: "/home/ubuntu/xiaoshen/code/IsaacLab-new/GR-Isaaclab/outputs/ppt_learning/outputs/pcd/2025-05-25_16-46-21/ur5_put_bowl_in_microwave_and_close" # pretrained model path for testing
  model_names: model_500.pth
  freeze_trunk: False # whether to freeze the trunk during finetuning
prompt: ""
domains: ur5_put_bowl_in_microwave_and_close
stem:
  pointcloud: 
    pretrained_path: "/mnt/xiaoshen/scanobjectnn-pointnext-s_best.pth"